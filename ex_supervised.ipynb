{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear discriminant : pink vs green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding a LDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_p = 2\n",
    "mu_g = -2\n",
    "sigma = 1\n",
    "number_points = 20\n",
    "pink = np.random.normal(mu_p,sigma,number_points)\n",
    "green = np.random.normal(mu_g,sigma,number_points)\n",
    "n = 2*number_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual \"input\" data X is given by the pink and green arrays. Here we have drawn inputs for the two categries (pink and green) from gaussian distributions with the same variance but different means. This actually corresponds to the hypothesis we make in the LDA, i.e. we assume that inside each category, samples are gaussianly distributed. We could define it as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.concatenate((green,pink),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"output\" vector y is then y = ('green', 'green', ... , 'pink','pink') with 20 x 'green' and 20 x 'pink'. We could also assign an integer to each category. Let's say that 'green' = 0 and 'pink' = 1 for instance. Then y = (0,0,...,1,1,...). \n",
    "\n",
    "Estimate of the means for both categories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_p_hat=\n",
    "mu_g_hat="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the variance for both categories, and for 'sigma^2' (see slides):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_hat = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate pi_green and pi_pink: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi_green = \n",
    "pi_pink = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the boundary value for decision: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writting delta_pink = delta_green gives us the boundary value for an new observation \n",
    "#to be classified as green or pink \n",
    "\n",
    "x_boundary = \n",
    "x_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.concatenate((green,pink),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram (just need to have defined x_boundary and the plotting will work):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(10)  # deterministic random data\n",
    "bins=np.linspace(-20,20,100)\n",
    "bins=np.append(bins,np.max([np.max(pink),np.max(green)]))\n",
    "a = np.hstack((pink,green))\n",
    "plt.ylim(top=np.max(bins))\n",
    "y1, x1,_ = plt.hist(pink, bins='auto',alpha=.4,color='pink',label='pink histogram')  # alpha level => transparent color (in case histog. overlap)\n",
    "y2, x2,_ = plt.hist(green,bins='auto',alpha=.4,color='green',label='green histogram')\n",
    "plt.ylim([0,1.1*np.max(np.array([y1,y2]))])\n",
    "plt.axvline(x_boundary,label='boundary value')\n",
    "plt.title(\"Histogram\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Assessing the training error \n",
    "\n",
    "Using the threshold value of x we computed, we can see to which category all samples would be assigned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# all 'green' elements in our labelled data set have the following \"x\" values: \n",
    "green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us look which of those would been correctly assigned with our obtained threshold: (use green < x_boundary and np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The number is the 'output' cell just above is the number of correctly classified 'green' elements. \n",
    "\n",
    "Similarly, we will have the following number of correclty classified pink samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us focus on the green category. We can compute the \n",
    "\n",
    "- true positive rate : number of correclty classified green samples. \n",
    "- false positive rate : number of samples classified as green while they are not green. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "true_positive_rate = \n",
    "print('true positive rate is ',true_positive_rate)\n",
    "\n",
    "false_positive_rate = \n",
    "print('false positive rate is ',false_positive_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs are X are encoded in the training_data array defined above and the outputs can be defined as follow (because by construction the green guys are the 20 first entries and the pink one the 20 last): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate((np.zeros(20),np.ones(20)),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use \n",
    "\n",
    "clf = LDA() \n",
    "\n",
    "clf.fit(X, y)Â \n",
    "\n",
    "and then the function clf.predict to predict the category of a new input (what you need to define): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The simplest neural network ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def nonlin(x, deriv=False):\n",
    "    if (deriv == True):\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# input dataset\n",
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "# output dataset\n",
    "y = np.array([[0, 0, 1, 1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2 * np.random.random((3, 1)) - 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a loop over 10 000 iteration to update the weights for a ONE LAYER neural network with 1 ouput in the loop you should start by evaluating the output with the current weights then you compute the error made\n",
    "to update your weights, you can use the 'gradient method' finally you need to update your weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LOOCV (illustration)\n",
    "\n",
    "Because the sampling of the pink and green distributions above do not overlap, the estimation of the test MSE will be very good. For the purpose of the exercise, we will use pink and green distributions with means values -1 and 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_p = 1.0\n",
    "mu_g = -1.0\n",
    "sigma = 1\n",
    "number_points = 20\n",
    "pink = np.random.normal(mu_p,sigma,number_points)\n",
    "green = np.random.normal(mu_g,sigma,number_points)\n",
    "n = 2*number_points\n",
    "mu_p_hat=np.mean(pink)\n",
    "mu_g_hat=np.mean(green)\n",
    "pink_n = pink - mu_p_hat\n",
    "green_n = green - mu_g_hat\n",
    "sigma_hat = 1/(n-1)*(np.dot(pink_n.T,pink_n)+np.dot(green_n.T,green_n))\n",
    "training_data = np.concatenate((green,pink),axis=0)\n",
    "my_outputs=np.ndarray.flatten(np.array([np.repeat('pink',3),np.repeat('green',3)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(10)  # deterministic random data\n",
    "bins=np.linspace(-20,20,100)\n",
    "bins=np.append(bins,np.max([np.max(pink),np.max(green)]))\n",
    "a = np.hstack((pink,green))\n",
    "plt.ylim(top=np.max(bins))\n",
    "y1, x1,_ = plt.hist(pink, bins='auto',alpha=.4,color='pink',label='pink histogram')  # alpha level => transparent color (in case histog. overlap)\n",
    "y2, x2,_ = plt.hist(green,bins='auto',alpha=.4,color='green',label='green histogram')\n",
    "plt.ylim([0,1.1*np.max(np.array([y1,y2]))])\n",
    "plt.axvline(x_boundary,label='boundary value')\n",
    "plt.title(\"Histogram\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will now loop over each observation to estimate the \"test MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#initialize my correct and incorrect counters\n",
    "correct_eval = 0\n",
    "incorrect_eval = 0 \n",
    "\n",
    "for kk in range(len(training_data)):\n",
    "\n",
    "    #for each training set consisting of all observations but observation \"kk\" \n",
    "    #we will estimate whether or not the \"test set\", ie the observation \"kk\" is correctly classified\n",
    "    #!! the following works only for data set with first half of the data set belonging to class 1, and second half to class 2\n",
    "    indices = [i for i in range(number_points) if i != kk]\n",
    "    if kk < number_points:\n",
    "        my_outputs_green = green[indices]\n",
    "        my_outputs_pink =  pink\n",
    "    else: \n",
    "        my_outputs_green = green\n",
    "        my_outputs_pink = pink[indices]\n",
    "    mu_pink = np.mean(my_outputs_pink)\n",
    "    mu_green = np.mean(my_outputs_green)\n",
    "    threshold = (mu_pink+mu_green)/2\n",
    "\n",
    "    if kk < number_points: \n",
    "        if training_data[kk] < threshold:\n",
    "            correct_eval = correct_eval + 1\n",
    "        else:\n",
    "            incorrect_eval = incorrect_eval + 1\n",
    "    else:\n",
    "        if training_data[kk] > threshold:\n",
    "            correct_eval = correct_eval + 1\n",
    "        else:\n",
    "            incorrect_eval = incorrect_eval + 1\n",
    "            \n",
    "print(correct_eval)\n",
    "print(incorrect_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The evaluation of the equivalent of test MSE for classications problem is the percentage of correctly evaluated observations \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "correct_eval/(correct_eval+incorrect_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
